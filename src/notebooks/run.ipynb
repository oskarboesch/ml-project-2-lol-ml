{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Mean Squared Error (MSE): 0.0055733822286191115\n",
      "Validation R² Score: 0.25550055443814357\n",
      "Test predictions saved to 'test_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from utils import load_data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Load the data\n",
    "data_train, data_test , targets_train= load_data(True)\n",
    "\n",
    "# Preprocess the data\n",
    "# Drop the \"Unnamed: 0\" column (cell line identifiers) as it's not a feature\n",
    "X = data_train.drop(columns=[\"Unnamed: 0\"])\n",
    "y = targets_train[\"AAC\"]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "random_forest = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred = random_forest.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse)\n",
    "print(\"Validation R² Score:\", r2)\n",
    "\n",
    "# Predict on test data\n",
    "X_test = data_test.drop(columns=[\"Unnamed: 0\"])\n",
    "y_test_pred = random_forest.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      8\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../utils\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m     12\u001b[0m data_train, data_test , targets_train\u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Bureau\\ml-project-2-lol-ml\\src\\notebooks\\../utils\\utils.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, categorical \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from utils import load_data\n",
    "\n",
    "# Load the data\n",
    "data_train, data_test , targets_train= load_data(True)\n",
    "\n",
    "# Preprocess the data\n",
    "# Drop the \"Unnamed: 0\" column (cell line identifiers) as it's not a feature\n",
    "X = data_train.drop(columns=[\"Unnamed: 0\"])\n",
    "y = targets_train[\"AAC\"]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid with more options for a thorough search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],  # Wider range for number of trees\n",
    "    'max_depth': [5, 10, 15, 20, 30, None],     # Include None for unrestricted depth\n",
    "    'min_samples_split': [2, 5, 10],            # Minimum samples for a split\n",
    "    'min_samples_leaf': [1, 2, 4, 8],           # Leaf node regularization\n",
    "    'max_features': ['sqrt', 'log2']            # Avoid 'auto' (deprecated in sklearn)\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                              # Increase cross-validation folds\n",
    "    scoring='neg_mean_squared_error',  # Try alternative scoring metrics\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract best parameters and evaluate\n",
    "best_model = grid_search.best_estimator_\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Validation set evaluation\n",
    "y_val_pred = best_model.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse)\n",
    "print(\"Validation R² Score:\", r2)\n",
    "\n",
    "# Test set predictions\n",
    "X_test = data_test.drop(columns=[\"Unnamed: 0\"])\n",
    "y_test_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Mean Squared Error (MSE): 0.005764102991137482\n",
      "Validation R² Score: 0.2300238338172358\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from utils import load_data\n",
    "\n",
    "# Load the data\n",
    "data_train, data_test , targets_train= load_data(True)\n",
    "\n",
    "# Drop the \"Unnamed: 0\" column and separate features and target\n",
    "X = data_train.drop(columns=[\"Unnamed: 0\"])\n",
    "y = targets_train[\"AAC\"]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an initial Random Forest model to get feature importances\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances and select the top 500 features\n",
    "importances = rf.feature_importances_\n",
    "important_indices = np.argsort(importances)[::-1][:500]  # Top 500 important features\n",
    "X_train_reduced = X_train.iloc[:, important_indices]\n",
    "X_val_reduced = X_val.iloc[:, important_indices]\n",
    "\n",
    "# Train a new Random Forest model on the reduced feature set\n",
    "rf_reduced = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "rf_reduced.fit(X_train_reduced, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_val_pred = rf_reduced.predict(X_val_reduced)\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse)\n",
    "print(\"Validation R² Score:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Mean Squared Error (MSE): 0.006829186175894043\n",
      "Validation R² Score: 0.08774867521484764\n",
      "Test predictions saved to 'test_predictions_xgb.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "from utils import load_data\n",
    "\n",
    "# Load the data\n",
    "data_train, data_test , targets_train= load_data(True)\n",
    "\n",
    "# Preprocess the data\n",
    "# Drop the \"Unnamed: 0\" column (cell line identifiers) as it's not a feature\n",
    "X = data_train.drop(columns=[\"Unnamed: 0\"])\n",
    "y = targets_train[\"AAC\"]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an XGBoost model\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=100,       # Number of trees\n",
    "    learning_rate=0.05,     # Learning rate\n",
    "    max_depth=6,            # Maximum depth of each tree\n",
    "    random_state=42,\n",
    "    n_jobs=-1               # Use all available CPU cores\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation data\n",
    "y_val_pred = xgb_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse)\n",
    "print(\"Validation R² Score:\", r2)\n",
    "\n",
    "# Predict on test data\n",
    "X_test = data_test.drop(columns=[\"Unnamed: 0\"])\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Save predictions\n",
    "test_predictions = pd.DataFrame({\"AAC_Predicted\": y_test_pred})\n",
    "test_predictions.to_csv(\"test_predictions_xgb.csv\", index=False)\n",
    "\n",
    "print(\"Test predictions saved to 'test_predictions_xgb.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      9\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../utils\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_data\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m     13\u001b[0m data_train, data_test , targets_train\u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Bureau\\ml-project-2-lol-ml\\src\\notebooks\\../utils\\utils.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, categorical \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "#use MLP to predict the AAC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "\n",
    "from utils import load_data\n",
    "\n",
    "# Load the data\n",
    "data_train, data_test , targets_train= load_data(True)\n",
    "\n",
    "# Preprocess the data\n",
    "# Drop the \"Unnamed: 0\" column (cell line identifiers) as it's not a feature\n",
    "X = data_train.drop(columns=[\"Unnamed: 0\"])\n",
    "y = targets_train[\"AAC\"]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train an MLP model\n",
    "mlp_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50),  # Two hidden layers with 100 and 50 neurons\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "mlp_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation data\n",
    "\n",
    "y_val_pred = mlp_model.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "\n",
    "mse = mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "\n",
    "print(\"Validation Mean Squared Error (MSE):\", mse)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
